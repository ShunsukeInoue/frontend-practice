ドキュメント1（D1）: DifyのRAG設定・知識ベース運用（50行）
	•	D1-01: Difyではナレッジベースにファイルを取り込み、チャンク化して検索対象にする。
	•	D1-02: Difyのチャンクサイズは短すぎると文脈が欠け、長すぎるとノイズが増える。
	•	D1-03: Difyではチャンクのオーバーラップを増やすと境界の情報欠落を減らせる。
	•	D1-04: DifyはVector SearchとFull-text SearchとHybrid Searchのモードを選択できる。
	•	D1-05: Hybrid Searchではキーワード重みとセマンティック重みを調整できる。
	•	D1-06: Difyは検索結果に対してRerank Modelを適用して順位を再計算できる。
	•	D1-07: DifyのScore Thresholdは類似度が低いチャンクを除外するために使う。
	•	D1-08: DifyはTopKを増やすと再現率が上がる一方で精度が落ちやすい。
	•	D1-09: Difyはメタデータフィルタで部署IDや公開範囲を指定して検索対象を絞れる。
	•	D1-10: Difyは同義語が多い質問ほどHybrid検索の恩恵を受けやすい。
	•	D1-11: DifyではEmbeddingモデルを変えると近傍検索の分布が大きく変わる。
	•	D1-12: DifyはRAGで取得したチャンクを回答プロンプトに挿入して根拠を与える。
	•	D1-13: Difyはコンテキスト数が多いと回答が散りやすいのでTopK制御が重要になる。
	•	D1-14: DifyはRerankを入れるとTopKが同じでも上位の品質が改善することがある。
	•	D1-15: Difyでは「親子チャンク」方式で親文書を保持し、子チャンクで検索する構成がある。
	•	D1-16: 親子チャンクでは検索は子を返し、表示や要約は親を返す設計にすると読みやすい。
	•	D1-17: Difyはナレッジ更新時に古いチャンクの削除と再投入の整合性が課題になる。
	•	D1-18: Difyはドキュメントごとにタグを付けて、質問時にフィルタ条件として使える。
	•	D1-19: Difyは表記ゆれ対策として正規化や辞書展開を前処理に入れられる。
	•	D1-20: Difyはタイトルや見出しをメタデータとして保持すると検索の説明性が上がる。
	•	D1-21: DifyはPDFやHTML取り込み時に改行や箇条書きの崩れがチャンク品質に影響する。
	•	D1-22: Difyは「全文モード」のように広めに取得してから要約する設計も取れる。
	•	D1-23: DifyのHybrid検索は全文検索エンジンとベクタ検索を内部で統合している。
	•	D1-24: Difyは検索結果のスコアをUI上で確認でき、しきい値調整の材料にできる。
	•	D1-25: Difyは同じ質問でも温度や再ランキング設定で取得順が変わることがある。
	•	D1-26: Difyではナレッジベースを複数に分け、用途ごとに検索範囲を限定できる。
	•	D1-27: Difyはクエリ拡張を使うと、短い質問でも関連語を補って検索できる。
	•	D1-28: DifyはRAGの失敗を「未取得」と「誤取得」に分けて改善すると効率が良い。
	•	D1-29: Difyは会話履歴をクエリに混ぜると関連性が上がる場合と下がる場合がある。
	•	D1-30: Difyはファイル単位のアクセス制御を設計するとBtoB利用で事故が減る。
	•	D1-31: Difyは回答生成より先に「取得の妥当性」を検査するステップを挟める。
	•	D1-32: Difyは質問の言語と文書の言語が違うとEmbeddingが弱くなることがある。
	•	D1-33: Difyはチャンクに文書IDと行番号を埋めるとデバッグが速くなる。
	•	D1-34: Difyは「出典表示」を前提に、チャンクにURLやファイル名を持たせる。
	•	D1-35: Difyはベクタ検索だけだと数字や固有名詞で取りこぼすケースがある。
	•	D1-36: Difyは全文検索だけだと同義語や言い換えに弱いケースがある。
	•	D1-37: DifyはHybridで両者の弱点を補い、質問タイプで重みを変えられる。
	•	D1-38: Difyはスコアしきい値を上げすぎると何も取得できない質問が増える。
	•	D1-39: Difyは「取得0件」の場合にフォールバック回答を用意してUXを守る。
	•	D1-40: DifyはRerankにより上位のノイズが減るが、コストと遅延が増える。
	•	D1-41: DifyはTopKを大きくしてRerankで絞る戦略が取りやすい。
	•	D1-42: Difyはメタデータで版数や施行日を管理すると古い規程を避けられる。
	•	D1-43: Difyは評価用に「質問→期待チャンク」をCSVで管理すると回しやすい。
	•	D1-44: Difyは同じ文書でもチャンク境界の違いで検索ヒットが変わる。
	•	D1-45: Difyは「質問の意図」を分類して検索モードを切り替えると精度が上がる。
	•	D1-46: DifyはRAG結果をログに残して、誤取得パターンを後で分析できる。
	•	D1-47: Difyは知識ベースのサイズが増えるほど、フィルタとRerankの重要度が増す。
	•	D1-48: DifyはEmbedding次元や距離尺度を変えると近傍の密度が変化する。
	•	D1-49: Difyはユースケース別に「取得の深さ」と「回答の簡潔さ」を設計する。
	•	D1-50: Difyは運用で文書追加が続くため、再インデックス手順を手順書化する。

⸻

ドキュメント2（D2）: S3 Vectors前提のRAG実装・検索API設計（50行）
	•	D2-01: S3 Vectorsはベクタを保存し、近傍検索でTopKを返す仕組みとして使える。
	•	D2-02: S3 Vectorsはドキュメント単位の概念を持たないのでメタデータで管理する。
	•	D2-03: doc_idをメタデータに入れると、後で文書ごとに集計やデバッグができる。
	•	D2-04: chunk_idを一意にし、doc_idとline_noを紐付けると再現性が上がる。
	•	D2-05: S3 VectorsはFull-text検索を提供しないので、必要なら別エンジンが要る。
	•	D2-06: S3 VectorsはHybrid検索の重み付けを持たないので、スコア融合はアプリ側で行う。
	•	D2-07: S3 VectorsはRerank機能を持たないので、再ランキングは外部モデルで行う。
	•	D2-08: 類似度しきい値はQuery結果のdistanceを見てアプリ側で足切りする。
	•	D2-09: S3 Vectorsはメタデータフィルタで検索対象の属性を絞れる。
	•	D2-10: メタデータにdepartment_idを入れると部署限定検索のテストができる。
	•	D2-11: メタデータにeffective_dateを入れると最新規程だけを検索する設計ができる。
	•	D2-12: TopKを増やすと候補が広がるが、ノイズも増えるので後段の絞り込みが重要になる。
	•	D2-13: QueryVectorsでreturnDistanceを有効にすると距離が返り、しきい値調整に使える。
	•	D2-14: QueryVectorsでreturnMetadataを有効にするとdoc_idなどの属性が返る。
	•	D2-15: S3 Vectorsの結果をそのままLLMに渡すと長文化しやすいので要約や圧縮が要る。
	•	D2-16: S3 Vectorsではチャンク分割戦略は外部で決めてからベクタを投入する。
	•	D2-17: 1文1チャンクにするとゴールデンセットが作りやすいが、文脈不足も起きやすい。
	•	D2-18: オーバーラップを使う場合は重複チャンクが増えるので重複除去が必要になる。
	•	D2-19: 同じ文書を再投入する場合はversionをメタデータに入れて切り替える。
	•	D2-20: S3 Vectorsの運用では削除や更新のAPI手順が重要になる。
	•	D2-21: ベクタ投入前に正規化や表記ゆれ統一をすると検索の安定性が上がる。
	•	D2-22: ベクタの次元数はEmbeddingモデルに依存し、保管側は生ベクタを扱う。
	•	D2-23: 近傍検索は語彙一致が弱いので数字や型番はメタデータで補うと良い。
	•	D2-24: 固有名詞の揺れは別名辞書でクエリ拡張すると拾いやすくなる。
	•	D2-25: Queryのベクタ化はアプリ側で実行し、S3 Vectorsにはベクタを渡す。
	•	D2-26: 同じ質問でもEmbeddingが変わると近傍が変わるので比較実験が必要になる。
	•	D2-27: 取得後にcross-encoderでRerankすると上位の関連性が改善しやすい。
	•	D2-28: 取得後にkeywordスコアとdistanceを融合すると疑似Hybridが作れる。
	•	D2-29: S3 Vectorsは検索の説明性が低いのでmetadataとログで補う。
	•	D2-30: distanceの分布を収集すると、しきい値の初期値を合理的に決められる。
	•	D2-31: doc_id別にTop1率を集計すると、特定文書への偏りを検出できる。
	•	D2-32: 同一doc_idのチャンクが上位を独占する場合は多様性制御が必要になる。
	•	D2-33: 多様性制御はdoc_idごとに上限を設けるなどアプリ側で実装する。
	•	D2-34: 取得チャンク数の上限は、LLMの入力トークン制限に合わせて決める。
	•	D2-35: 取得後に重複文を除外すると、文脈が偏るのを防げる。
	•	D2-36: 取得0件の場合はdistanceの閾値を緩めて再検索するフォールバックが使える。
	•	D2-37: 取得が多すぎる場合は上位Nだけ要約してLLMに渡す戦略がある。
	•	D2-38: メタデータにsource_titleを入れると出典表示が簡単になる。
	•	D2-39: メタデータにsource_urlを入れると回答に根拠リンクを出せる。
	•	D2-40: セキュア運用ではIAM権限を最小化し、投入と検索の権限を分ける。
	•	D2-41: バッチ投入は失敗時のリトライ設計と冪等性が重要になる。
	•	D2-42: 検索のレイテンシはTopKやフィルタ条件で変わるので計測が必要になる。
	•	D2-43: 評価時は同じEmbeddingモデルと同じ前処理で比較しないと差が読めない。
	•	D2-44: RAGのテストでは「質問→期待chunk_id」の正解表が最重要になる。
	•	D2-45: 期待チャンクが複数ある質問はTopK評価とRecall評価が必要になる。
	•	D2-46: S3 Vectors単体ではログUIがないので検索結果をアプリで保存する。
	•	D2-47: distanceとmetadataを保存すると誤取得の原因分析がしやすくなる。
	•	D2-48: 類似文が多いコーパスではEmbeddingの差が小さくなり順位が揺れやすい。
	•	D2-49: 順位が揺れる場合はRerankや多様性制御で安定化できる。
	•	D2-50: S3 VectorsでDify相当を狙うなら検索戦略をアプリ側で積み上げる必要がある。

⸻

ドキュメント3（D3）: RAGテスト設計・評価指標・チューニング（50行）
	•	D3-01: RAGのテストは「質問」と「期待するチャンク」のゴールデンセットを作ることから始まる。
	•	D3-02: 期待チャンクはTop1だけでなくTopKで定義すると現実のRAGに近づく。
	•	D3-03: 1文1チャンクのデータは人間の正解付けが簡単で検証が速い。
	•	D3-04: 類似文が多いテストでは順位の入れ替わりが起きやすい。
	•	D3-05: 検索評価はRecall@KとPrecision@Kを同時に見ると誤解が減る。
	•	D3-06: TopKを増やすとRecall@Kが上がるがPrecision@Kが下がりやすい。
	•	D3-07: Rerankは上位のPrecisionを上げるが計算コストと遅延が増える。
	•	D3-08: Hybrid検索は数値や固有名詞の取りこぼしを減らすのに有効なことが多い。
	•	D3-09: キーワード寄りに重みを振ると一致性は上がるが言い換えに弱くなる。
	•	D3-10: セマンティック寄りに重みを振ると言い換えには強いが誤ヒットが増えることがある。
	•	D3-11: 類似度しきい値は「取得0件」と「誤取得」のバランスで調整する。
	•	D3-12: しきい値を上げすぎると答えがあるのに取得できないケースが増える。
	•	D3-13: しきい値を下げすぎるとノイズが混ざって回答が不安定になる。
	•	D3-14: distance分布を観察してしきい値の初期値を決めると手戻りが減る。
	•	D3-15: 評価は同一の質問セットを固定してA/B比較しないと意味が薄れる。
	•	D3-16: 文書をまたいで答えが必要な質問は、統合検索の実力を測りやすい。
	•	D3-17: doc_idで多様性制御を入れると単一文書への偏りを抑えられる。
	•	D3-18: 多様性制御は「doc_idごとに最大N件」など簡単な制約でも効く。
	•	D3-19: チャンク境界が悪いと関連文が分断され、Recallが落ちる。
	•	D3-20: チャンク境界が粗いと無関係文が混ざり、Precisionが落ちる。
	•	D3-21: オーバーラップはRecallを上げるが重複が増えてPrecisionを下げることがある。
	•	D3-22: 取得後の重複除去はLLMの入力を節約し、回答の偏りも減らせる。
	•	D3-23: 取得チャンクをそのまま渡すより、要約してから渡す方が安定する場合がある。
	•	D3-24: ただし要約は情報落ちが起きるので評価セットで効果検証が必要になる。
	•	D3-25: 検索の揺れを抑えるには前処理の正規化とEmbedding固定が効く。
	•	D3-26: Embeddingを変えた比較は、同じチャンク分割と同じ質問セットで行う。
	•	D3-27: クエリ拡張は短い質問のRecallを上げるが誤拡張でノイズも増える。
	•	D3-28: 失敗事例は「未取得」「誤取得」「取得は良いが生成が悪い」に分類する。
	•	D3-29: 「取得は良いが生成が悪い」はプロンプトや引用形式の改善が効く。
	•	D3-30: 「誤取得」はメタデータフィルタやRerankの追加で改善しやすい。
	•	D3-31: 「未取得」はチャンク設計やHybrid化で改善しやすい。
	•	D3-32: 数字や型番中心の質問は全文検索やメタデータ検索が強い。
	•	D3-33: 意味中心の質問はベクタ検索が強いが類似文が多いと迷いやすい。
	•	D3-34: 評価ログには質問、TopK、distance、doc_id、正誤を保存すると良い。
	•	D3-35: Top1一致率だけを見るとTop2〜Top5の改善が見えなくなる。
	•	D3-36: TopKの中に正解が入るかを見て、Rerankの必要性を判断できる。
	•	D3-37: Rerank前後で同じTopK候補集合を使うと比較が公平になる。
	•	D3-38: Hybridの重みは質問タイプ別に最適値が違うので分類が役立つ。
	•	D3-39: フィルタ条件が強すぎるとRecallが落ち、弱すぎるとPrecisionが落ちる。
	•	D3-40: ドキュメントの版管理が弱いと古いチャンクが混ざり誤回答の原因になる。
	•	D3-41: effective_dateやversionをメタデータに入れると版混在を防げる。
	•	D3-42: 出典表示があると誤取得を人間が素早く発見できる。
	•	D3-43: ゴールデンセットは小さく作って高速に回し、徐々に拡張する。
	•	D3-44: 10問の小セットでもTopKとしきい値の方向性は掴める。
	•	D3-45: 検索が安定しない場合は類似文の密度を下げるかRerankを追加する。
	•	D3-46: 取得候補を増やしてRerankで絞るとTop1率が上がりやすい。
	•	D3-47: 取得候補が増えすぎると入力トークンが溢れるので圧縮が必要になる。
	•	D3-48: 同一質問を複数回投げ、順位の分散を見ると安定性評価になる。
	•	D3-49: 安定性が低い場合は前処理、Embedding、Rerank、フィルタの順で疑う。
	•	D3-50: 最終的に重要なのは「ユーザーが欲しい根拠がTopKに入る確率」である。

⸻

質問10個 + 期待取得チャンクTop10（関連度順）

Q1: Hybrid検索の重みをどう調整すれば、言い換えと固有名詞の両方に強くできますか？
	1.	D1-05: Hybrid Searchではキーワード重みとセマンティック重みを調整できる。
	2.	D1-37: DifyはHybridで両者の弱点を補い、質問タイプで重みを変えられる。
	3.	D3-08: Hybrid検索は数値や固有名詞の取りこぼしを減らすのに有効なことが多い。
	4.	D3-09: キーワード寄りに重みを振ると一致性は上がるが言い換えに弱くなる。
	5.	D3-10: セマンティック寄りに重みを振ると言い換えには強いが誤ヒットが増えることがある。
	6.	D2-06: S3 VectorsはHybrid検索の重み付けを持たないので、スコア融合はアプリ側で行う。
	7.	D2-28: 取得後にkeywordスコアとdistanceを融合すると疑似Hybridが作れる。
	8.	D1-10: Difyは同義語が多い質問ほどHybrid検索の恩恵を受けやすい。
	9.	D1-35: Difyはベクタ検索だけだと数字や固有名詞で取りこぼすケースがある。
	10.	D3-38: Hybridの重みは質問タイプ別に最適値が違うので分類が役立つ。

Q2: S3 Vectorsで「ドキュメント単位」を扱いたい場合、どう設計するのが良いですか？
	1.	D2-02: S3 Vectorsはドキュメント単位の概念を持たないのでメタデータで管理する。
	2.	D2-03: doc_idをメタデータに入れると、後で文書ごとに集計やデバッグができる。
	3.	D2-04: chunk_idを一意にし、doc_idとline_noを紐付けると再現性が上がる。
	4.	D2-14: QueryVectorsでreturnMetadataを有効にするとdoc_idなどの属性が返る。
	5.	D3-17: doc_idで多様性制御を入れると単一文書への偏りを抑えられる。
	6.	D2-33: 多様性制御はdoc_idごとに上限を設けるなどアプリ側で実装する。
	7.	D1-33: Difyはチャンクに文書IDと行番号を埋めるとデバッグが速くなる。
	8.	D2-31: doc_id別にTop1率を集計すると、特定文書への偏りを検出できる。
	9.	D3-41: effective_dateやversionをメタデータに入れると版混在を防げる。
	10.	D2-38: メタデータにsource_titleを入れると出典表示が簡単になる。

Q3: 類似度しきい値（Score Threshold / distance閾値）はどう決めれば良いですか？
	1.	D1-07: DifyのScore Thresholdは類似度が低いチャンクを除外するために使う。
	2.	D2-08: 類似度しきい値はQuery結果のdistanceを見てアプリ側で足切りする。
	3.	D3-11: 類似度しきい値は「取得0件」と「誤取得」のバランスで調整する。
	4.	D3-12: しきい値を上げすぎると答えがあるのに取得できないケースが増える。
	5.	D3-13: しきい値を下げすぎるとノイズが混ざって回答が不安定になる。
	6.	D3-14: distance分布を観察してしきい値の初期値を決めると手戻りが減る。
	7.	D2-30: distanceの分布を収集すると、しきい値の初期値を合理的に決められる。
	8.	D1-38: Difyはスコアしきい値を上げすぎると何も取得できない質問が増える。
	9.	D2-36: 取得0件の場合はdistanceの閾値を緩めて再検索するフォールバックが使える。
	10.	D1-39: Difyは「取得0件」の場合にフォールバック回答を用意してUXを守る。

Q4: Rerankを入れると何が改善し、どんなコストが増えますか？
	1.	D1-06: Difyは検索結果に対してRerank Modelを適用して順位を再計算できる。
	2.	D1-14: DifyはRerankを入れるとTopKが同じでも上位の品質が改善することがある。
	3.	D3-07: Rerankは上位のPrecisionを上げるが計算コストと遅延が増える。
	4.	D1-40: DifyはRerankにより上位のノイズが減るが、コストと遅延が増える。
	5.	D1-41: DifyはTopKを大きくしてRerankで絞る戦略が取りやすい。
	6.	D2-07: S3 VectorsはRerank機能を持たないので、再ランキングは外部モデルで行う。
	7.	D2-27: 取得後にcross-encoderでRerankすると上位の関連性が改善しやすい。
	8.	D3-36: TopKの中に正解が入るかを見て、Rerankの必要性を判断できる。
	9.	D3-46: 取得候補を増やしてRerankで絞るとTop1率が上がりやすい。
	10.	D3-37: Rerank前後で同じTopK候補集合を使うと比較が公平になる。

Q5: チャンクサイズとオーバーラップは、何を基準に決めれば良いですか？
	1.	D1-02: Difyのチャンクサイズは短すぎると文脈が欠け、長すぎるとノイズが増える。
	2.	D1-03: Difyではチャンクのオーバーラップを増やすと境界の情報欠落を減らせる。
	3.	D3-19: チャンク境界が悪いと関連文が分断され、Recallが落ちる。
	4.	D3-20: チャンク境界が粗いと無関係文が混ざり、Precisionが落ちる。
	5.	D3-21: オーバーラップはRecallを上げるが重複が増えてPrecisionを下げることがある。
	6.	D2-16: S3 Vectorsではチャンク分割戦略は外部で決めてからベクタを投入する。
	7.	D2-17: 1文1チャンクにするとゴールデンセットが作りやすいが、文脈不足も起きやすい。
	8.	D1-44: Difyは同じ文書でもチャンク境界の違いで検索ヒットが変わる。
	9.	D3-22: 取得後の重複除去はLLMの入力を節約し、回答の偏りも減らせる。
	10.	D3-03: 1文1チャンクのデータは人間の正解付けが簡単で検証が速い。

Q6: TopKを増やすと精度と再現率はどう変わり、何を見て調整すべきですか？
	1.	D1-08: DifyはTopKを増やすと再現率が上がる一方で精度が落ちやすい。
	2.	D3-06: TopKを増やすとRecall@Kが上がるがPrecision@Kが下がりやすい。
	3.	D2-12: TopKを増やすと候補が広がるが、ノイズも増えるので後段の絞り込みが重要になる。
	4.	D1-13: Difyはコンテキスト数が多いと回答が散りやすいのでTopK制御が重要になる。
	5.	D3-05: 検索評価はRecall@KとPrecision@Kを同時に見ると誤解が減る。
	6.	D3-35: Top1一致率だけを見るとTop2〜Top5の改善が見えなくなる。
	7.	D3-36: TopKの中に正解が入るかを見て、Rerankの必要性を判断できる。
	8.	D1-41: DifyはTopKを大きくしてRerankで絞る戦略が取りやすい。
	9.	D2-34: 取得チャンク数の上限は、LLMの入力トークン制限に合わせて決める。
	10.	D3-47: 取得候補が増えすぎると入力トークンが溢れるので圧縮が必要になる。

Q7: 部署限定検索をしたい場合、メタデータフィルタはどう使うべきですか？
	1.	D1-09: Difyはメタデータフィルタで部署IDや公開範囲を指定して検索対象を絞れる。
	2.	D2-09: S3 Vectorsはメタデータフィルタで検索対象の属性を絞れる。
	3.	D2-10: メタデータにdepartment_idを入れると部署限定検索のテストができる。
	4.	D3-39: フィルタ条件が強すぎるとRecallが落ち、弱すぎるとPrecisionが落ちる。
	5.	D1-42: Difyはメタデータで版数や施行日を管理すると古い規程を避けられる。
	6.	D2-11: メタデータにeffective_dateを入れると最新規程だけを検索する設計ができる。
	7.	D3-41: effective_dateやversionをメタデータに入れると版混在を防げる。
	8.	D2-14: QueryVectorsでreturnMetadataを有効にするとdoc_idなどの属性が返る。
	9.	D2-29: S3 Vectorsは検索の説明性が低いのでmetadataとログで補う。
	10.	D3-34: 評価ログには質問、TopK、distance、doc_id、正誤を保存すると良い。

Q8: RAGのゴールデンセット（質問→期待チャンク）を作る手順は？
	1.	D3-01: RAGのテストは「質問」と「期待するチャンク」のゴールデンセットを作ることから始まる。
	2.	D2-44: RAGのテストでは「質問→期待chunk_id」の正解表が最重要になる。
	3.	D1-43: Difyは評価用に「質問→期待チャンク」をCSVで管理すると回しやすい。
	4.	D3-02: 期待チャンクはTop1だけでなくTopKで定義すると現実のRAGに近づく。
	5.	D3-43: ゴールデンセットは小さく作って高速に回し、徐々に拡張する。
	6.	D3-44: 10問の小セットでもTopKとしきい値の方向性は掴める。
	7.	D3-15: 評価は同一の質問セットを固定してA/B比較しないと意味が薄れる。
	8.	D2-45: 期待チャンクが複数ある質問はTopK評価とRecall評価が必要になる。
	9.	D3-34: 評価ログには質問、TopK、distance、doc_id、正誤を保存すると良い。
	10.	D3-28: 失敗事例は「未取得」「誤取得」「取得は良いが生成が悪い」に分類する。

Q9: S3 Vectorsで距離（distance）とメタデータを返して、閾値調整に使いたいです。
	1.	D2-13: QueryVectorsでreturnDistanceを有効にすると距離が返り、しきい値調整に使える。
	2.	D2-14: QueryVectorsでreturnMetadataを有効にするとdoc_idなどの属性が返る。
	3.	D2-30: distanceの分布を収集すると、しきい値の初期値を合理的に決められる。
	4.	D3-14: distance分布を観察してしきい値の初期値を決めると手戻りが減る。
	5.	D2-47: distanceとmetadataを保存すると誤取得の原因分析がしやすくなる。
	6.	D3-11: 類似度しきい値は「取得0件」と「誤取得」のバランスで調整する。
	7.	D2-08: 類似度しきい値はQuery結果のdistanceを見てアプリ側で足切りする。
	8.	D3-34: 評価ログには質問、TopK、distance、doc_id、正誤を保存すると良い。
	9.	D2-29: S3 Vectorsは検索の説明性が低いのでmetadataとログで補う。
	10.	D1-24: Difyは検索結果のスコアをUI上で確認でき、しきい値調整の材料にできる。

Q10: 親子チャンク構成で「検索は子、表示は親」にしたい場合の設計ポイントは？
	1.	D1-15: Difyでは「親子チャンク」方式で親文書を保持し、子チャンクで検索する構成がある。
	2.	D1-16: 親子チャンクでは検索は子を返し、表示や要約は親を返す設計にすると読みやすい。
	3.	D3-19: チャンク境界が悪いと関連文が分断され、Recallが落ちる。
	4.	D3-20: チャンク境界が粗いと無関係文が混ざり、Precisionが落ちる。
	5.	D2-02: S3 Vectorsはドキュメント単位の概念を持たないのでメタデータで管理する。
	6.	D2-03: doc_idをメタデータに入れると、後で文書ごとに集計やデバッグができる。
	7.	D2-04: chunk_idを一意にし、doc_idとline_noを紐付けると再現性が上がる。
	8.	D3-17: doc_idで多様性制御を入れると単一文書への偏りを抑えられる。
	9.	D3-22: 取得後の重複除去はLLMの入力を節約し、回答の偏りも減らせる。
	10.	D1-34: Difyは「出典表示」を前提に、チャンクにURLやファイル名を持たせる。
